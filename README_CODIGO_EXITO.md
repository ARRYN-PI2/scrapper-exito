# üìã An√°lisis del C√≥digo - Exito E-commerce Scraper

Este documento explica en detalle qu√© hace el c√≥digo dentro de este repositorio y c√≥mo funciona internamente.

## üéØ Prop√≥sito del Sistema

El **Exito E-commerce Scraper** es un sistema profesional dise√±ado para extraer informaci√≥n estructurada de productos del sitio web de √âxito (exito.com), una de las principales cadenas de retail en Colombia.

### Objetivos Principales:
- ‚úÖ Extraer datos de productos de manera automatizada
- ‚úÖ Limpiar y estructurar la informaci√≥n obtenida
- ‚úÖ Generar datasets en m√∫ltiples formatos (JSON, JSONL, CSV)
- ‚úÖ Facilitar an√°lisis de mercado y comparaci√≥n de precios

---

## ¬øQu√© hace este c√≥digo?

### Funcionalidades Principales

1. **Extracci√≥n de Productos**: Obtiene informaci√≥n detallada de productos de diferentes categor√≠as disponibles en Exito.com

2. **M√∫ltiples Estrategias de Scraping**:
   - **Estado JSON Embebido** (Recomendado): Extrae objetos JavaScript con datos estructurados del HTML (SSR/SPA state)
   - **HTML Scraping** (Fallback): Parsing tradicional del DOM usando BeautifulSoup
   - **API VTEX** (Indirecto): Aprovecha la infraestructura VTEX que usa √âxito para su e-commerce

3. **Formatos de Salida Flexibles**:
    *Guardados en scrapper-exito/data*
   - **JSONL**: Un producto por l√≠nea en formato JSON (eficiente para big data)
   - **CSV**: Archivo estructurado compatible con Excel y herramientas de an√°lisis
   
   El formato JSONL es procesado por la clase `exito_scraper/adapters/json_repo.py` que toma los datos y genera autom√°ticamente un archivo JSON formateado m√°s legible (`*_formatted.json`) que se exporta a `/data/` para facilitar la lectura humana.

4. **Categor√≠as Soportadas**: 7 categor√≠as de productos incluyendo televisores, celulares, electrodom√©sticos, audio, videojuegos y deportes.

5. **Limpieza Autom√°tica de Datos**:
   - **HTML Sanitization**: Convierte descripciones HTML a texto plano legible
   - **Extracci√≥n de Precios**: Separa texto de precio y valor num√©rico
   - **Normalizaci√≥n**: Estructura consistente de datos independientemente de la fuente

### Flujo de Ejecuci√≥n Paso a Paso

```
1. üöÄ INICIO
   ‚îú‚îÄ Usuario ejecuta comando con categor√≠a y n√∫mero de p√°ginas
   ‚îî‚îÄ Sistema valida par√°metros de entrada

2. üîß CONFIGURACI√ìN
   ‚îú‚îÄ Carga URL base de la categor√≠a desde config.py
   ‚îú‚îÄ Inicializa adaptadores (Scraper + Repositorio)
   ‚îî‚îÄ Configura headers HTTP y rate limiting

3. üîÑ ITERACI√ìN POR P√ÅGINAS
   Para cada p√°gina (1 a N):
   
   3.1 üì° SOLICITUD HTTP
       ‚îú‚îÄ Construye URL con par√°metro de p√°gina
       ‚îú‚îÄ Aplica delay aleatorio (1-2 segundos)
       ‚îú‚îÄ Realiza request con headers realistas
       ‚îî‚îÄ Valida respuesta HTTP exitosa
   
   3.2 üîç ESTRATEGIA DE PARSING
       ‚îú‚îÄ Intenta extraer JSON embebido del HTML
       ‚îÇ   ‚îú‚îÄ Busca patrones: __STATE__, __APOLLO_STATE__, __NEXT_DATA__
       ‚îÇ   ‚îî‚îÄ Si encuentra: procesa datos estructurados ‚úÖ
       ‚îÇ
       ‚îî‚îÄ Si falla: usa parsing HTML tradicional
           ‚îú‚îÄ Parsea DOM con BeautifulSoup
           ‚îú‚îÄ Extrae productos de grillas/tarjetas
           ‚îî‚îÄ Obtiene datos individuales por producto
   
   3.3 üßπ LIMPIEZA DE DATOS
       Para cada producto encontrado:
       ‚îú‚îÄ Limpia HTML de descripciones
       ‚îú‚îÄ Extrae precio num√©rico del texto
       ‚îú‚îÄ Normaliza campos opcionales
       ‚îú‚îÄ Asigna metadatos (fecha, p√°gina, contador)
       ‚îî‚îÄ Valida estructura del objeto Producto
   
   3.4 üíæ PERSISTENCIA
       ‚îú‚îÄ Agrupa productos de la p√°gina
       ‚îú‚îÄ Escribe a archivo JSONL (una l√≠nea por producto)
       ‚îî‚îÄ Actualiza contadores globales

4. üìä POSTPROCESAMIENTO
   ‚îú‚îÄ Genera archivo JSON formateado para lectura humana
   ‚îú‚îÄ Aplica indentaci√≥n y estructura legible
   ‚îî‚îÄ Guarda como *_formatted.json en /data/

5. ‚úÖ FINALIZACI√ìN
   ‚îú‚îÄ Reporta estad√≠sticas de extracci√≥n
   ‚îú‚îÄ Cierra conexiones HTTP
   ‚îî‚îÄ Retorna c√≥digo de estado del proceso
```

### Orden de Eventos Interno

```
ExitoScraperAdapter ‚Üí ScrapeCategoryUseCase ‚Üí Repositories
     ‚Üì                        ‚Üì                    ‚Üì
1. scrape()              2. run()            3. persist()
   ‚îú‚îÄ _with_page()          ‚îú‚îÄ for pages         ‚îú‚îÄ write JSONL
   ‚îú‚îÄ _get()                ‚îî‚îÄ scraper.scrape()  ‚îî‚îÄ format JSON
   ‚îú‚îÄ _extract_state_json()
   ‚îú‚îÄ _parse_html_grid()
   ‚îî‚îÄ return productos[]
```

---

## üèóÔ∏è Arquitectura del Sistema

### Patr√≥n Arquitect√≥nico: **Hexagonal (Puertos y Adaptadores)**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Adaptadores  ‚îÇ    ‚îÇ    Aplicaci√≥n   ‚îÇ    ‚îÇ     Dominio     ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ Exito       ‚îÇ ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Scrape      ‚îÇ ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Producto    ‚îÇ ‚îÇ
‚îÇ ‚îÇ Scraper     ‚îÇ ‚îÇ    ‚îÇ ‚îÇ UseCase     ‚îÇ ‚îÇ    ‚îÇ ‚îÇ (Entity)    ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ                 ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ JSON/CSV    ‚îÇ ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚î§                 ‚îÇ    ‚îÇ ‚îÇ Ports       ‚îÇ ‚îÇ
‚îÇ ‚îÇ Repository  ‚îÇ ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ ‚îÇ (Interfaces)‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ    ‚îÇ                 ‚îÇ    ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìÅ Estructura del C√≥digo

### **1. Dominio (`exito_scraper/domain/`)**

#### `producto.py` - Entidad Principal
```python
@dataclass
class Producto:
    contador_extraccion_total: int
    contador_extraccion: int
    titulo: str
    marca: str
    precio_texto: str
    precio_valor: Optional[int]
    moneda: Optional[str]
    tama√±o: str
    calificacion: str
    detalles_adicionales: str
    fuente: str
    categoria: str
    imagen: str
    link: str
    pagina: int
    fecha_extraccion: str
    extraction_status: str = "OK"
```

**Responsabilidades:**
- Definir la estructura de datos de un producto
- Validar que todos los campos requeridos est√©n presentes
- Proporcionar m√©todos de serializaci√≥n (`to_dict()`)
- Generar timestamps autom√°ticos (`now_iso()`)

#### `ports.py` - Contratos de Interfaces
```python
class ScraperPort(Protocol):
    def scrape(self, categoria: str, page: int) -> Iterable[Producto]: ...

class RepositoryPort(Protocol):
    def persist(self, productos: Iterable[Producto]) -> None: ...
```

**Responsabilidades:**
- Definir contratos que deben cumplir los adaptadores
- Garantizar independencia del dominio respecto a implementaciones externas

### **2. Aplicaci√≥n (`exito_scraper/application/`)**

#### `scrape_usecase.py` - Caso de Uso Principal
```python
class ScrapeCategoryUseCase:
    def run(self, categoria: str, pages: int = 1) -> None:
        for p in range(1, pages + 1):
            productos = list(self.scraper.scrape(categoria, p))
            if not productos:
                continue
            self.repo.persist(productos)
```

**Responsabilidades:**
- Coordinar el proceso de extracci√≥n
- Iterar sobre m√∫ltiples p√°ginas
- Manejar casos donde no hay productos
- Persistir los resultados obtenidos

### **3. Adaptadores (`exito_scraper/adapters/`)**

#### `exito_scraper_adapter.py` - Scraper Principal
**Funciones clave:**

1. **Manejo de URLs:**
```python
def _with_page(self, url: str, page: int) -> str:
    # Modifica par√°metros de p√°gina en la URL
```

2. **Control de Rate Limiting:**
```python
def _sleep(self):
    lo, hi = REQUEST_DELAY_SECONDS
    time.sleep(random.uniform(lo, hi))  # 1-2 segundos entre requests
```

3. **Extracci√≥n de Estado JSON:**
```python
def _extract_state_json(self, html: str) -> Optional[dict]:
    # Busca objetos JSON embebidos en el HTML (SSR/SPA state)
```

4. **Parsing HTML Tradicional:**
```python
def _parse_html_grid(self, html: str, categoria: str, page: int) -> List[Producto]:
    # Fallback usando BeautifulSoup para extraer productos del DOM
```

#### `json_repo.py` y `csv_repo.py` - Repositorios de Datos
**Responsabilidades:**
- Persistir productos en formatos espec√≠ficos
- JSONL: Una l√≠nea por producto (eficiente para big data)
- JSON: Formato estructurado y legible
- CSV: Compatible con Excel y an√°lisis estad√≠stico

### **4. Utilidades (`exito_scraper/utils/`)**

#### `html_formatter.py` - Limpieza de HTML
```python
def clean_html_details(html_content: str) -> str:
    # Convierte HTML a texto plano
    # Elimina tags, espacios extra, etc.
```

---

## ‚öôÔ∏è Configuraci√≥n (`exito_scraper/config.py`)

### **Categor√≠as Soportadas:**
```python
EXPECTED_URLS = {
    "televisores": "https://www.exito.com/tecnologia/televisores?...",
    "celulares": "https://www.exito.com/tecnologia/celulares?...",
    "lavadoras": "https://www.exito.com/electrodomesticos/lavado-y-secado?...",
    "refrigeracion": "https://www.exito.com/electrodomesticos/refrigeracion?...",
    "audio": "https://www.exito.com/tecnologia/audio?...",
    "videojuegos": "https://www.exito.com/tecnologia/consolas-y-videojuegos?...",
    "deportes": "https://www.exito.com/deportes-y-fitness?..."
}
```

### **Configuraci√≥n de Red:**
- **Headers:** User-Agent realista para evitar detecci√≥n
- **Timeout:** 25 segundos por request
- **Rate Limiting:** 1-2 segundos entre requests
- **Idioma:** es-CO (espa√±ol Colombia)

---

## üîÑ Flujo de Ejecuci√≥n

### **1. Inicializaci√≥n**
```python
# main.py
scraper = ExitoScraperAdapter()        # Adaptador de scraping
repo = _make_repo(args.output)         # Repositorio (JSON/CSV)
usecase = ScrapeCategoryUseCase(scraper, repo)  # Caso de uso
```

### **2. Proceso de Extracci√≥n**
```
Para cada p√°gina (1 to N):
  ‚îå‚îÄ Construir URL con par√°metro de p√°gina
  ‚îú‚îÄ Realizar request HTTP con delay
  ‚îú‚îÄ Parsear HTML/JSON response
  ‚îú‚îÄ Extraer productos individuales
  ‚îú‚îÄ Limpiar HTML en descripciones
  ‚îú‚îÄ Crear objetos Producto
  ‚îî‚îÄ Persistir en formato seleccionado
```

### **3. Estrategias de Parsing**
1. **Preferida:** Extraer estado JSON embebido (m√°s eficiente)
2. **Fallback:** Parsing HTML tradicional con BeautifulSoup

---

## üìä Datos Extra√≠dos

### **Por cada producto se obtiene:**

| Campo | Descripci√≥n | Ejemplo |
|-------|-------------|---------|
| `titulo` | Nombre del producto | "Smart TV Samsung 55\" UHD 4K" |
| `marca` | Marca del fabricante | "Samsung" |
| `precio_texto` | Precio como aparece | "$1.299.900" |
| `precio_valor` | Precio num√©rico | 1299900 |
| `moneda` | C√≥digo de moneda | "COP" |
| `calificacion` | Rating del producto | "4.5/5" |
| `detalles_adicionales` | Descripci√≥n limpia | "Pantalla LED con HDR" |
| `imagen` | URL de imagen | "https://..." |
| `link` | URL del producto | "https://..." |
| `categoria` | Categor√≠a extra√≠da | "televisores" |
| `fecha_extraccion` | Timestamp ISO | "2025-09-25T10:30:00" |

---

## üê≥ Containerizaci√≥n

### **Docker Multi-stage Build:**
```dockerfile
# Imagen base optimizada
FROM python:3.11-slim as base

# Dependencias del sistema
RUN apt-get update && apt-get install -y \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# Dependencias Python
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# C√≥digo de aplicaci√≥n
COPY exito_scraper/ ./exito_scraper/
```

### **Docker Compose para desarrollo:**
```yaml
services:
  exito-scraper:
    volumes:
      - ./data:/app/data              # Persistencia de datos
      - ./exito_scraper:/app/exito_scraper:ro  # Hot reload
```

---

## üöÄ Casos de Uso

### **1. An√°lisis de Mercado**
```bash
# Extraer 5 p√°ginas de televisores
docker-compose run --rm exito-scraper scrape --categoria televisores --paginas 5
```

### **2. Monitoreo de Precios**
```bash
# Obtener datos actuales para comparar con hist√≥ricos
python -m exito_scraper.main scrape --categoria celulares --paginas 3 --output data/celulares_$(date +%Y%m%d).jsonl
```

### **3. Investigaci√≥n de Productos**
```bash
# Datos para ML o an√°lisis estad√≠stico
docker run --rm -v $(pwd)/data:/app/data exito-scraper scrape --categoria audio --paginas 2
```

---

## üõ°Ô∏è Robustez y Manejo de Errores

### **Estrategias Implementadas:**
- ‚úÖ **Rate limiting** para evitar ser bloqueado
- ‚úÖ **User-Agent realista** para parecer navegador real
- ‚úÖ **Timeouts configurables** para manejar conexiones lentas
- ‚úÖ **Fallback de parsing** (JSON ‚Üí HTML)
- ‚úÖ **Continuaci√≥n en p√°ginas vac√≠as** (tolerancia a intermitencias)
- ‚úÖ **Limpieza autom√°tica de HTML** en contenido
- ‚úÖ **Validaci√≥n de datos** en el modelo de dominio

---

## üìà M√©tricas y Tracking

### **Contadores Implementados:**
- `contador_extraccion_total`: Productos totales extra√≠dos en la sesi√≥n
- `contador_extraccion`: Productos por p√°gina
- `extraction_status`: Estado de extracci√≥n ("OK", "ERROR", etc.)
- `fecha_extraccion`: Timestamp para auditor√≠a

---

## üîß Extensibilidad

### **Para agregar nuevas categor√≠as:**
1. A√±adir URL en `config.py`
2. No requiere cambios de c√≥digo adicionales

### **Para nuevos formatos de salida:**
1. Implementar `RepositoryPort`
2. Registrar en `main.py`

### **Para nuevos sitios web:**
1. Implementar `ScraperPort`
2. Mantener la misma interfaz de dominio

---

## üìã Resumen

Este scraper es un **sistema empresarial robusto** que combina:
- **Arquitectura limpia** para mantenibilidad
- **Estrategias de parsing h√≠bridas** para confiabilidad
- **Containerizaci√≥n** para despliegue consistente
- **Rate limiting √©tico** para no sobrecargar el servidor
- **M√∫ltiples formatos** para diferentes an√°lisis
- **Documentaci√≥n completa** para diferentes roles de usuario

Es ideal para **analistas de datos**, **investigadores de mercado** y **desarrolladores** que necesitan datos estructurados del e-commerce de √âxito de manera automatizada y confiable.
